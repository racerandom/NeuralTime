{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyknp import Juman\n",
    "import utils\n",
    "from model import *\n",
    "import torch\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_URL = \"checkpoints/ALL_20200830171210/cv0/\"\n",
    "excel_file = \"行動データ3言語54subs (002).xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphologicalAnalyzer(object):\n",
    "\n",
    "    def __init__(self, analyzer_name='juman'):\n",
    "        self.analyzer_name = analyzer_name\n",
    "        if self.analyzer_name == 'juman':\n",
    "            from pyknp import Juman\n",
    "            self.analyzer = Juman()\n",
    "        elif self.analyzer_name == 'mecab':\n",
    "            import MeCab\n",
    "            self.analyzer = MeCab.Tagger()\n",
    "\n",
    "    def analyze(self, text):\n",
    "        if self.analyzer_name == 'juman':\n",
    "            return [w.midasi for w in self.analyzer.analysis(text).mrph_list()]\n",
    "        elif self.analyzer_name == 'mecab':\n",
    "            return self.analyzer.parse(text).split()\n",
    "\n",
    "def analyze_morph_and_mask(sent, analyzer, tokenizer):\n",
    "    tokenized_sent = []\n",
    "    event_mask = []\n",
    "    for item_id, item in enumerate(sent):\n",
    "        analyzed_item = ' '.join(analyzer.analyze(item))\n",
    "        tokenized_item = tokenizer.tokenize(analyzed_item)\n",
    "        tokenized_sent += tokenized_item\n",
    "        if item_id != 2:\n",
    "            event_mask += [0] * len(tokenized_item)\n",
    "        else:\n",
    "            event_mask += [1] * len(tokenized_item)\n",
    "    \n",
    "    assert len(tokenized_sent) == len(event_mask)\n",
    "    tokenized_sent = ['[CLS]'] + tokenized_sent + ['[SEP]']\n",
    "    event_mask = [0] + event_mask + [0]\n",
    "    return tokenized_sent, event_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = pd.read_excel(excel_file, encoding=\"SHIFT-JIS\")\n",
    "sentences = in_data.values[1:-6,[6,7,8]].tolist()\n",
    "sentences[0]\n",
    "\n",
    "ma = MorphologicalAnalyzer()\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_URL,\n",
    "    do_lower_case=False, \n",
    "    do_basic_tokenize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_toks = [analyze_morph_and_mask(sent, ma, tokenizer)[0] for sent in sentences]\n",
    "event_masks = [analyze_morph_and_mask(sent, ma, tokenizer)[1] for sent in sentences]\n",
    "\n",
    "max_len = max([len(sent) for sent in sent_toks])\n",
    "padded_sent_toks = utils.padding_2d(sent_toks, max_len, pad_tok='[PAD]')\n",
    "padded_event_masks = utils.padding_2d(event_masks, max_len, pad_tok=0)\n",
    "attn_masks = [[1] * len(sent) + [0] * (max_len - len(sent)) for sent in sent_toks]\n",
    "padded_sent_tids = [tokenizer.convert_tokens_to_ids(sent) for sent in padded_sent_toks]\n",
    "\n",
    "test_tensors = TensorDataset(\n",
    "    torch.tensor(padded_sent_tids),\n",
    "    torch.tensor(padded_event_masks),\n",
    "    torch.tensor(attn_masks)\n",
    ")\n",
    "\n",
    "test_dataset = DataLoader(test_tensors, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file(os.path.join(MODEL_URL, 'config.json'))\n",
    "model = DocEmbMultiTaskTRC(\n",
    "    config,\n",
    "    num_emb=2,\n",
    "    task_list=['DCT', 'T2E', 'E2E', 'MAT'],\n",
    "    num_labels=len(tag2id)\n",
    ")\n",
    "state_dict = torch.load(os.path.join(MODEL_URL, 'pytorch_model.bin'))\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {'AFTER': 4, 'OVERLAP-OR-AFTER': 3, 'OVERLAP': 2, 'BEFORE-OR-OVERLAP': 1, 'BEFORE': 0, 'VAGUE': 5}\n",
    "id2tag = {v: k for k, v in tag2id.items()}\n",
    "output_ids = []\n",
    "model.eval()\n",
    "for b_tok_ids, b_event_mask, b_attn_mask in test_dataset:\n",
    "    b_tok_ids = b_tok_ids.to(device)\n",
    "    b_event_mask = b_event_mask.to(device)\n",
    "    b_attn_mask = b_attn_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        b_pred_logits = model(\n",
    "            b_tok_ids, \n",
    "            b_event_mask, \n",
    "            b_event_mask, \n",
    "            'DCT', \n",
    "            attention_mask=b_attn_mask, \n",
    "            labels=None\n",
    "        )\n",
    "        output_ids += torch.argmax(b_pred_logits, dim=-1).squeeze(1).cpu().detach().tolist()\n",
    "outputs = [id2tag[tag_id] for tag_id in output_ids]\n",
    "\n",
    "in_data.insert(9, 'DCT Tag', ['DCT Tag'] + outputs + ([''] * 6))\n",
    "in_data.to_excel('行動データ3言語54subs (002)_tagged.xlsx', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, tag in zip(sent_toks, outputs):\n",
    "    print(sent, tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bert_env]",
   "language": "python",
   "name": "conda-env-bert_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
