{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pyknp import Juman\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "import mojimoji\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GroupShuffleSplit, KFold\n",
    "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
    "import logging\n",
    "from model import RelationClassifier\n",
    "\n",
    "from utils import *\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if str(device) == 'cpu':\n",
    "    BERT_URL='/Users/fei-c/Resources/embed/L-12_H-768_A-12_E-30_BPE'\n",
    "elif str(device) == 'cuda':\n",
    "    BERT_URL='/larch/share/bert/Japanese_models/Wikipedia/L-12_H-768_A-12_E-30_BPE'\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_URL, do_lower_case=False, do_basic_tokenize=False)\n",
    "juman = Juman()\n",
    "\n",
    "logger = logging.getLogger('Data_Process')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "if (logger.hasHandlers()):\n",
    "    logger.handlers.clear()\n",
    "# add ch to logger\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_map_6c = {\n",
    "    'after': 'AFTER',\n",
    "    'met-by': 'AFTER',\n",
    "    'overlapped-by': 'OVERLAP-OR-AFTER',\n",
    "    'finishes': 'OVERLAP-OR-AFTER',\n",
    "    'during': 'OVERLAP',\n",
    "    'started-by': 'OVERLAP',\n",
    "    'equal' : 'OVERLAP',\n",
    "    'starts': 'BEFORE-OR-OVERLAP',\n",
    "    'contains': 'OVERLAP',\n",
    "    'finished-by' : 'OVERLAP',\n",
    "    'overlaps' : 'BEFORE-OR-OVERLAP',\n",
    "    'meets' : 'BEFORE',\n",
    "    'before': 'BEFORE',\n",
    "    'is_included' : 'OVERLAP',\n",
    "    'identity' : 'OVERLAP',\n",
    "    'includes' : 'OVERLAP',\n",
    "    'vague' : 'VAGUE',\n",
    "}\n",
    "merge_map_4c = {\n",
    "    'after': 'AFTER',\n",
    "    'met-by': 'AFTER',\n",
    "    'overlapped-by': 'OVERLAP',\n",
    "    'finishes': 'OVERLAP',\n",
    "    'during': 'OVERLAP',\n",
    "    'started-by': 'OVERLAP',\n",
    "    'equal' : 'OVERLAP',\n",
    "    'starts': 'OVERLAP',\n",
    "    'contains': 'OVERLAP',\n",
    "    'finished-by' : 'OVERLAP',\n",
    "    'overlaps' : 'OVERLAP',\n",
    "    'meets' : 'BEFORE',\n",
    "    'before': 'BEFORE',\n",
    "    'is_included' : 'OVERLAP',\n",
    "    'identity' : 'OVERLAP',\n",
    "    'includes' : 'OVERLAP',\n",
    "    'vague' : 'VAGUE',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, checkpoint_dir):\n",
    "\n",
    "    # Goal: Save a model, configuration and vocabulary that you have fine-tuned\n",
    "    \n",
    "    # If we have a distributed model, save only the encapsulated model\n",
    "    # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "    # If we save using the predefined names, we can load using `from_pretrained`\n",
    "    checkpoint_model_file = os.path.join(checkpoint_dir, WEIGHTS_NAME)\n",
    "    checkpoint_config_file = os.path.join(checkpoint_dir, CONFIG_NAME)\n",
    "\n",
    "    torch.save(model_to_save.state_dict(), checkpoint_model_file)\n",
    "    model_to_save.config.to_json_file(checkpoint_config_file)\n",
    "    tokenizer.save_vocabulary(checkpoint_dir)\n",
    "\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def merge_lab(lab, lab_type='6c'):\n",
    "    \n",
    "    # Goal: merge 13+1 labels into 5+1 or 3+1 labels.\n",
    "    if lab_type == '6c':\n",
    "        return merge_map_6c[lab] if lab in merge_map_6c else 'VAGUE'\n",
    "    elif lab_type == '4c':\n",
    "        return merge_map_4c[lab] if lab in merge_map_4c else 'VAGUE'\n",
    "\n",
    "    \n",
    "def vote_labels(lab_a, lab_b, lab_c, lab_type=None, comp=None):\n",
    "    \n",
    "    if lab_type in ['4c', '6c']:\n",
    "        lab_a = merge_lab(lab_a, lab_type = lab_type)\n",
    "        lab_b = merge_lab(lab_b, lab_type = lab_type)\n",
    "        lab_c = merge_lab(lab_c, lab_type = lab_type)\n",
    "    \n",
    "    if comp:\n",
    "        if lab_a == lab_b == lab_c:\n",
    "            return lab_a\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        if lab_a == lab_b or lab_a == lab_c:\n",
    "            return lab_a\n",
    "        elif lab_b == lab_c:\n",
    "            return lab_b\n",
    "        else:\n",
    "            return 'vague'\n",
    "\n",
    "    \n",
    "def mask_mention(sent_mask, mention_id):\n",
    "    return [1 if mask == mention_id else 0 for mask in sent_mask]\n",
    "\n",
    "\n",
    "def extract_sents_from_xml(xml_file, verbose):\n",
    "    doc_deunk_toks, doc_toks, doc_masks, doc_tlinks = [], [], [], []\n",
    "    eiid2eid = {}\n",
    "    tlinks = defaultdict(lambda: dict())\n",
    "    root = ET.parse(xml_file).getroot()\n",
    "    for text_node in root.findall('TEXT'):\n",
    "        for dct_node in text_node.find('TIMEX3'):\n",
    "            pass\n",
    "        for event_node in text_node.iter('MAKEINSTANCE'):\n",
    "            eiid2eid[event_node.attrib['eiid']] = event_node.attrib['eventID']\n",
    "        logger.info('event num: %i' % len(eiid2eid))\n",
    "        \n",
    "        for tlink_node in text_node.iter('TLINK'):\n",
    "            if tlink_node.attrib['task'] in ['DCT', 'T2E']:\n",
    "                tlink_key = '%s-%s' % (tlink_node.attrib['timeID'], \n",
    "                                       eiid2eid[tlink_node.attrib['relatedToEventInstance']])\n",
    "            elif tlink_node.attrib['task'] in ['E2E', 'MAT']:\n",
    "                tlink_key = '%s-%s' % (eiid2eid[tlink_node.attrib['eventInstanceID']], \n",
    "                                       eiid2eid[tlink_node.attrib['relatedToEventInstance']])\n",
    "            tlinks[tlink_node.attrib['task']][tlink_key] = vote_labels(tlink_node.attrib['relTypeA'],\n",
    "                                                                       tlink_node.attrib['relTypeB'],\n",
    "                                                                       tlink_node.attrib['relTypeC'])\n",
    "        \n",
    "        for sent_node in text_node.iter('sentence'):\n",
    "            sent_toks, sent_masks = [], []\n",
    "            for tag in sent_node.iter():\n",
    "                try:\n",
    "                    if tag.text and tag.text.strip():\n",
    "                        text_seg = juman.analysis(mojimoji.han_to_zen(tag.text.strip()))\n",
    "                        sent_toks += [w.midasi for w in text_seg.mrph_list()]\n",
    "                        if tag.tag in ['EVENT', 'event'] and 'eid' in tag.attrib:\n",
    "                            sent_masks += [tag.attrib['eid']] * len(text_seg)\n",
    "                        elif tag.tag in ['TIMEX3'] and 'tid' in tag.attrib:\n",
    "                            sent_masks += [tag.attrib['tid']] * len(text_seg)\n",
    "                        else:\n",
    "                            sent_masks += ['O'] * len(text_seg)\n",
    "                    if tag.tag != 'sentence' and tag.tail and tag.tail.strip():\n",
    "                        tail_seg = juman.analysis(mojimoji.han_to_zen(tag.tail.strip()))\n",
    "                        sent_toks += [w.midasi for w in tail_seg.mrph_list()]\n",
    "                        sent_masks += ['O'] * len(tail_seg)\n",
    "                except Exception as ex:\n",
    "                    logger.info(xml_file, tag.tag, tag.text, tag.attrib)\n",
    "            \n",
    "            sbp_toks = tokenizer.tokenize(' '.join(sent_toks))\n",
    "            deunk_toks = explore_unk(sbp_toks, sent_toks)\n",
    "            sbp_masks = match_sbp_label(sbp_toks, sent_masks)\n",
    "            \n",
    "            if deunk_toks:\n",
    "                doc_deunk_toks.append(deunk_toks)\n",
    "                doc_toks.append(sbp_toks)\n",
    "                doc_masks.append(sbp_masks)\n",
    "            \n",
    "                if verbose:\n",
    "                    logger.debug(sent_toks, len(sent_toks))\n",
    "                    logger.debug(sent_masks, len(sent_masks))\n",
    "                    logger.debug(sbp_toks, len(sbp_toks))\n",
    "                    logger.debug(sbp_masks, len(sbp_masks))\n",
    "                    logger.debug()\n",
    "    \n",
    "    return doc_deunk_toks, doc_toks, doc_masks, tlinks\n",
    "\n",
    "\n",
    "def make_tlink_instances(doc_masks, doc_tlinks, task, verbose):\n",
    "    logger.info('tlink num: %i' % len(doc_tlinks[task]))\n",
    "    tlink_count = 0\n",
    "    doc_sour_masks, doc_targ_masks, doc_labs = [], [], []\n",
    "    for sent_mask in doc_masks:\n",
    "        sent_sour_masks, sent_targ_masks, sent_labs = [], [], []\n",
    "        for key in doc_tlinks[task].keys():\n",
    "            sour_id, targ_id = key.split('-')\n",
    "            if task in ['DCT'] and targ_id in sent_mask:\n",
    "                rel = doc_tlinks[task][key]\n",
    "                targ_mask = mask_mention(sent_mask, targ_id)\n",
    "                sent_sour_masks.append([0] * len(sent_mask))\n",
    "                sent_targ_masks.append(targ_mask)\n",
    "                sent_labs.append(rel)\n",
    "                if verbose:\n",
    "                    logger.debug('%s\\t%s' % (key, rel))\n",
    "                    logger.debug('%s' % targ_mask)\n",
    "                tlink_count += 1\n",
    "            elif task in ['T2E', 'E2E', 'MAT'] and sour_id in sent_mask and targ_id in sent_mask:\n",
    "                rel = doc_tlinks[task][key]\n",
    "                sour_mask = mask_mention(sent_mask, sour_id)\n",
    "                targ_mask = mask_mention(sent_mask, targ_id)\n",
    "                sent_sour_masks.append(sour_mask)\n",
    "                sent_targ_masks.append(targ_mask)\n",
    "                sent_labs.append(rel)\n",
    "                logger.debug('%s\\t%s' % (key, rel))\n",
    "                logger.debug('%s' % sour_mask)\n",
    "                logger.debug('%s' % targ_mask)\n",
    "                tlink_count += 1\n",
    "        doc_sour_masks.append(sent_sour_masks)\n",
    "        doc_targ_masks.append(sent_targ_masks)\n",
    "        doc_labs.append(sent_labs)\n",
    "    logger.info('generated tlink num: %i' % tlink_count)\n",
    "    return doc_sour_masks, doc_targ_masks, doc_labs, len(doc_tlinks[task]), tlink_count\n",
    "\n",
    "\n",
    "def flatten_tlink_instance(deunk_toks, toks, sour_masks, targ_masks, labs):\n",
    "    f_deunk_toks, f_toks = [], []\n",
    "    for det, t, ls in zip(deunk_toks, toks, labs):\n",
    "        f_deunk_toks += [det] * len(ls)\n",
    "        f_toks += [t] * len(ls)\n",
    "    f_sour_masks = [mask for sent in sour_masks for mask in sent]\n",
    "    f_targ_masks = [mask for sent in targ_masks for mask in sent]\n",
    "    f_labs = [mask for sent in labs for mask in sent]\n",
    "    assert len(f_deunk_toks) == len(f_toks) == len(f_sour_masks) == len(f_targ_masks) == len(f_labs)\n",
    "    return f_deunk_toks, f_toks, f_sour_masks, f_targ_masks, f_labs\n",
    "\n",
    "\n",
    "def batch_make_tlink_instances(data_dir, task='E2E', verbose=0):\n",
    "    \n",
    "    total_tlink_num, total_tlink_count = 0, 0\n",
    "    deunk_toks, toks, sour_masks, targ_masks, labs = [], [], [], [], []\n",
    "    for file in sorted(os.listdir(data_dir)):\n",
    "        if file.endswith(\".xml\"):\n",
    "            dir_file = os.path.join(data_dir, file)\n",
    "        \n",
    "        # sentence-level data\n",
    "        doc_deunk_toks, doc_toks, doc_masks, doc_tlinks = extract_sents_from_xml(\n",
    "            dir_file, \n",
    "            verbose\n",
    "        )\n",
    "        \n",
    "        # instance-level data\n",
    "        doc_sour_masks, doc_targ_masks, doc_labs, tlink_num, tlink_count = make_tlink_instances(\n",
    "            doc_masks, \n",
    "            doc_tlinks, \n",
    "            task,\n",
    "            verbose\n",
    "        )\n",
    "\n",
    "        assert len(doc_deunk_toks) == len(doc_masks) == len(doc_sour_masks) == len(doc_targ_masks) == len(doc_labs)\n",
    "        total_tlink_num += tlink_num\n",
    "        total_tlink_count += tlink_count\n",
    "        deunk_toks += doc_deunk_toks\n",
    "        toks += doc_toks\n",
    "        sour_masks += doc_sour_masks\n",
    "        targ_masks += doc_targ_masks\n",
    "        labs += doc_labs\n",
    "    logger.info('%i\\t%i' % (total_tlink_num, total_tlink_count))\n",
    "    logger.info('sent num: %i, data instance num: %i' % (len(deunk_toks), sum([len(d) for d in labs])))\n",
    "    \n",
    "    # flatten sentence-level to instance-level tlinks\n",
    "    f_deunk_toks, f_toks, f_sour_masks, f_targ_masks, f_labs = flatten_tlink_instance(deunk_toks, \n",
    "                                                                                      toks, \n",
    "                                                                                      sour_masks, \n",
    "                                                                                      targ_masks, \n",
    "                                                                                      labs)\n",
    "    \n",
    "    logger.info('flat data instance num: %i' % len(f_deunk_toks))\n",
    "    \n",
    "    return f_deunk_toks, f_toks, f_sour_masks, f_targ_masks, f_labs\n",
    "\n",
    "\n",
    "def convert_to_np(toks, sour_masks, targ_masks, labs, lab2ix):\n",
    "    max_len = max([len(t) for t in toks])\n",
    "    pad_tok_ids, pad_masks, pad_sm, pad_tm = [], [], [], []\n",
    "    for inst_tok, inst_sm, inst_tm, inst_lab in zip(toks, sour_masks, targ_masks, labs):\n",
    "        pad_inst_tok = padding_1d(['[CLS]'] + inst_tok, max_len + 1, pad_tok='[PAD]')\n",
    "        pad_inst_tok_ids = tokenizer.convert_tokens_to_ids(pad_inst_tok)\n",
    "        pad_inst_masks = padding_1d([1] * (len(inst_tok) + 1), max_len + 1, pad_tok=0)\n",
    "        pad_inst_sm = padding_1d([0] + inst_sm, max_len + 1, pad_tok=0)\n",
    "        pad_inst_tm = padding_1d([0] + inst_tm, max_len + 1, pad_tok=0)\n",
    "        pad_tok_ids.append(pad_inst_tok_ids)\n",
    "        pad_masks.append(pad_inst_masks)\n",
    "        pad_sm.append(pad_inst_sm)\n",
    "        pad_tm.append(pad_inst_tm)\n",
    "    lab_ids = [lab2ix[l] for l in labs]\n",
    "    assert len(pad_tok_ids) == len(pad_masks) == len(pad_sm) == len(pad_tm) == len(lab_ids)\n",
    "    return np.array(pad_tok_ids), np.array(pad_masks), np.array(pad_sm), np.array(pad_tm), np.array(lab_ids)\n",
    "\n",
    "\n",
    "def generate_group_ids(toks):\n",
    "    group_ids = []\n",
    "    id_offset = 1\n",
    "    for index, inst_tok in enumerate(toks):\n",
    "        if index > 0:\n",
    "            if np.array_equal(inst_tok, toks[index - 1]):\n",
    "                id_offset += 1\n",
    "        group_ids.append(id_offset)\n",
    "    assert len(toks) == len(group_ids)\n",
    "    return group_ids\n",
    "    \n",
    "    \n",
    "def merge_word_mention_boundaries(flat_word_ids, flat_doc_toks, mention_offsets):\n",
    "    \n",
    "    merge_num = 0\n",
    "    \n",
    "    flat_new_word_ids = flat_word_ids.copy()\n",
    "    \n",
    "    tmp_offs = 0\n",
    "    \n",
    "    for mid, mtype, offs_b, offs_e, m in mention_offsets:\n",
    "        print(merge_num, mid, mtype, offs_b, offs_e, m)\n",
    "        if offs_b == 0:\n",
    "            continue\n",
    "            \n",
    "        if flat_word_ids[offs_b - 1] == flat_word_ids[offs_b]:\n",
    "            print('b', m, flat_doc_toks[offs_b], flat_doc_toks[offs_b - 1], \n",
    "                  offs_b, offs_b - 1, \n",
    "                  flat_word_ids[offs_b - 1], flat_word_ids[offs_b])\n",
    "            while tmp_offs < offs_b:\n",
    "#                 print(tmp_offs, flat_doc_toks[tmp_offs], flat_word_ids[tmp_offs])\n",
    "                flat_new_word_ids[tmp_offs] = flat_word_ids[tmp_offs] + merge_num\n",
    "                print(flat_word_ids[tmp_offs] + merge_num)\n",
    "                tmp_offs += 1\n",
    "            merge_num += 1\n",
    "            \n",
    "        if flat_word_ids[offs_e - 1] == flat_word_ids[offs_e]:\n",
    "            print('e', m, flat_doc_toks[offs_e], flat_doc_toks[offs_e - 1], \n",
    "                  offs_e, offs_e - 1, \n",
    "                  flat_word_ids[offs_e - 1], flat_word_ids[offs_e])   \n",
    "            while tmp_offs < offs_e:\n",
    "#                 print(tmp_offs, flat_doc_toks[tmp_offs], flat_word_ids[tmp_offs])\n",
    "                flat_new_word_ids[tmp_offs] = flat_word_ids[tmp_offs] + merge_num\n",
    "                tmp_offs += 1\n",
    "            merge_num += 1\n",
    "        print(flat_word_ids[tmp_offs-1 if tmp_offs > 0 else 0] + merge_num)\n",
    "        print()\n",
    "    while tmp_offs < len(flat_word_ids):\n",
    "#         print(tmp_offs)\n",
    "        flat_new_word_ids[tmp_offs] = flat_word_ids[tmp_offs] + merge_num\n",
    "        tmp_offs += 1\n",
    "                \n",
    "    assert len(flat_word_ids) == len(flat_new_word_ids)\n",
    "            \n",
    "    return flat_new_word_ids\n",
    "\n",
    "\n",
    "def attach_word_ids(doc_words):\n",
    "    \n",
    "    word_ids = []\n",
    "    tmp_begin_id = 0\n",
    "    for sent_word in doc_words:\n",
    "        sent_ids = []\n",
    "        for word in sent_word:\n",
    "            sent_ids += [tmp_begin_id] * len(list(word))\n",
    "            tmp_begin_id += 1\n",
    "        word_ids.append(sent_ids)\n",
    "    return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_mention(sent_toks, ment_mask):\n",
    "    assert len(sent_toks) == len(ment_mask)\n",
    "    return [[t] for t, m in zip(sent_toks, ment_mask) if m == 1]\n",
    "\n",
    "\n",
    "def extract_sents_from_xml_v2(xml_file, lab_type=None, comp=None):\n",
    "    doc_deunk_toks, doc_toks = [], []\n",
    "    eiid2eid = {}\n",
    "    doc_mid2smask = {}\n",
    "    doc_tlinks = defaultdict(lambda:list())\n",
    "    root = ET.parse(xml_file).getroot()\n",
    "    for text_node in root.findall('TEXT'):\n",
    "        \n",
    "        for dct_node in text_node.find('TIMEX3'):\n",
    "            pass\n",
    "        \n",
    "        for event_node in text_node.iter('MAKEINSTANCE'):\n",
    "            eiid2eid[event_node.attrib['eiid']] = event_node.attrib['eventID']\n",
    "        logger.debug('event num: %i' % len(eiid2eid))\n",
    "        \n",
    "        for tlink_node in text_node.iter('TLINK'):\n",
    "            if tlink_node.attrib['task'] in ['DCT', 'T2E']:\n",
    "                sour_mid = tlink_node.attrib['timeID']\n",
    "                targ_mid = eiid2eid[tlink_node.attrib['relatedToEventInstance']]\n",
    "            elif tlink_node.attrib['task'] in ['E2E', 'MAT']:\n",
    "                sour_mid = eiid2eid[tlink_node.attrib['eventInstanceID']]\n",
    "                targ_mid = eiid2eid[tlink_node.attrib['relatedToEventInstance']]\n",
    "            voted_label = vote_labels(\n",
    "                tlink_node.attrib['relTypeA'],\n",
    "                tlink_node.attrib['relTypeB'],\n",
    "                tlink_node.attrib['relTypeC'],\n",
    "                lab_type=lab_type,\n",
    "                comp=comp\n",
    "            )\n",
    "            if voted_label:\n",
    "                doc_tlinks[tlink_node.attrib['task']].append((sour_mid, targ_mid, voted_label))\n",
    "        \n",
    "        s_id = 0\n",
    "        for sent_node in text_node.iter('sentence'):\n",
    "            logger.debug('sentence %i' % s_id)\n",
    "            sent_toks, tmp_mids = [], []\n",
    "            for tag in sent_node.iter():\n",
    "                try:\n",
    "                    if tag.text and tag.text.strip():\n",
    "                        text_seg = [w.midasi for w in juman.analysis(mojimoji.han_to_zen(tag.text.strip()).replace('\\u3000', '[JSP]')).mrph_list()]\n",
    "                        if tag.tag in ['EVENT', 'event'] and 'eid' in tag.attrib:\n",
    "                            tmp_mids.append(tag.attrib['eid'])\n",
    "                            doc_mid2smask[tag.attrib['eid']] = [\n",
    "                                s_id, \n",
    "                                [0] * len(sent_toks) + [1] * len(text_seg)\n",
    "                            ]\n",
    "                        elif tag.tag in ['TIMEX3'] and 'tid' in tag.attrib:\n",
    "                            tmp_mids.append(tag.attrib['tid'])\n",
    "                            doc_mid2smask[tag.attrib['tid']] = [\n",
    "                                s_id, \n",
    "                                [0] * len(sent_toks) + [1] * len(text_seg)\n",
    "                            ]\n",
    "                        sent_toks += text_seg\n",
    "                    if tag.tag != 'sentence' and tag.tail and tag.tail.strip():\n",
    "                        tail_seg = [w.midasi for w in juman.analysis(mojimoji.han_to_zen(tag.tail.strip()).replace('\\u3000', '[JSP]')).mrph_list()]\n",
    "                        sent_toks += tail_seg\n",
    "                except Exception as ex:\n",
    "                    logger.error(xml_file, tag.tag, tag.text, tag.attrib)\n",
    "                \n",
    "            \"\"\" subword tokenizer for word tokens \"\"\"\n",
    "            sbp_toks = tokenizer.tokenize(' '.join(sent_toks))\n",
    "            deunk_toks = explore_unk(sbp_toks, sent_toks)\n",
    "            logger.debug(str(len(sent_toks)) + ' ' + '/'.join(sent_toks))\n",
    "            logger.debug(str(len(deunk_toks)) + ' ' + '/'.join(deunk_toks))\n",
    "            \n",
    "            \"\"\" padding sentence mention masks with matching sbp tokens \"\"\"\n",
    "            for mid in tmp_mids:\n",
    "                sent_mask = padding_1d(doc_mid2smask[mid][1], len(sent_toks))\n",
    "                logger.debug('%s, sent_id: %i' % (mid, doc_mid2smask[mid][0]))\n",
    "                logger.debug(str(len(sent_mask)) + ' ' + ' '.join([str(i) for i in sent_mask]))\n",
    "                sbp_mask = match_sbp_mask(sbp_toks, sent_mask)\n",
    "                doc_mid2smask[mid][1] = sbp_mask\n",
    "                logger.debug(str(len(sbp_mask)) + ' ' + ' '.join([str(i) for i in sbp_mask]))\n",
    "                logger.debug(retrieve_mention(deunk_toks, sbp_mask))\n",
    "            \n",
    "            logger.debug('[EOS]')\n",
    "            \n",
    "            if deunk_toks:\n",
    "                doc_deunk_toks.append(deunk_toks)\n",
    "                doc_toks.append(sbp_toks)\n",
    "                s_id += 1\n",
    "    \n",
    "    return doc_deunk_toks, doc_toks, doc_mid2smask, doc_tlinks\n",
    "\n",
    "\n",
    "def make_tlink_instances_v2(doc_deunk_toks, doc_toks, doc_mid2smask, doc_tlinks, task=None):\n",
    "    deunk_toks, toks, sour_masks, targ_masks, sent_masks, rels = [], [], [], [], [], []\n",
    "    for sour_mid, targ_mid, rel in doc_tlinks[task]:\n",
    "        logger.debug('%s\\t%s\\t%s' % (sour_mid, targ_mid, rel))\n",
    "        targ_sid = doc_mid2smask[targ_mid][0]\n",
    "        if task in ['DCT']:\n",
    "            deunk_tok = doc_deunk_toks[targ_sid]\n",
    "            tok = doc_toks[targ_sid]\n",
    "            sour_mask = [0] * len(doc_mid2smask[targ_mid][1])\n",
    "            targ_mask = doc_mid2smask[targ_mid][1]\n",
    "            sent_mask = [0] * len(doc_mid2smask[targ_mid][1])\n",
    "        elif task in ['T2E', 'E2E', 'MAT']:\n",
    "            if sour_mid not in doc_mid2smask:\n",
    "                continue\n",
    "            sour_sid = doc_mid2smask[sour_mid][0]\n",
    "            if targ_sid - sour_sid == 0:\n",
    "                deunk_tok = doc_deunk_toks[targ_sid]\n",
    "                tok = doc_toks[targ_sid]\n",
    "                sour_mask = doc_mid2smask[sour_mid][1]\n",
    "                targ_mask = doc_mid2smask[targ_mid][1]\n",
    "                sent_mask = [0] * len(doc_mid2smask[targ_mid][1])\n",
    "            else:\n",
    "                deunk_tok = doc_deunk_toks[sour_sid] + doc_deunk_toks[targ_sid]\n",
    "                tok = doc_toks[sour_sid] + doc_toks[targ_sid]\n",
    "                sour_mask = doc_mid2smask[sour_mid][1] + [0] * len(doc_mid2smask[targ_mid][1])\n",
    "                targ_mask = [0] * len(doc_mid2smask[sour_mid][1]) + doc_mid2smask[targ_mid][1]\n",
    "                sent_mask = [0] * len(doc_mid2smask[sour_mid][1]) + [1] * len(doc_mid2smask[targ_mid][1])\n",
    "        \n",
    "        logger.debug(' '.join(deunk_tok)) \n",
    "        logger.debug(' '.join(tok)) \n",
    "        logger.debug(' '.join([str(i) for i in sour_mask])) \n",
    "        logger.debug(' '.join([str(i) for i in targ_mask])) \n",
    "        logger.debug(' '.join([str(i) for i in sent_mask])) \n",
    "        deunk_toks.append(deunk_tok)\n",
    "        toks.append(tok)\n",
    "        sour_masks.append(sour_mask)\n",
    "        targ_masks.append(targ_mask)\n",
    "        sent_masks.append(sent_mask)\n",
    "        rels.append(rel)\n",
    "        assert len(deunk_tok) == len(tok) == len(sour_mask) == len(targ_mask) == len(sent_mask)\n",
    "    return deunk_toks, toks, sour_masks, targ_masks, sent_masks, rels\n",
    "\n",
    "\n",
    "def batch_make_tlink_instances_v2(file_list, task=None, lab_type=None, comp=None):\n",
    "    deunk_toks, toks, sour_masks, targ_masks, sent_masks, rels = [], [], [], [], [], []\n",
    "    for dir_file in file_list:\n",
    "        logger.debug('[Done] processing %s' % dir_file)\n",
    "        doc_deunk_toks, doc_toks, doc_mid2smask, doc_tlinks = extract_sents_from_xml_v2(\n",
    "            dir_file,\n",
    "            lab_type=lab_type,\n",
    "            comp=comp\n",
    "        )\n",
    "        inst_deunk_toks, inst_toks, inst_sour_masks, inst_targ_masks, inst_sent_masks, inst_rels = make_tlink_instances_v2(\n",
    "            doc_deunk_toks, \n",
    "            doc_toks, \n",
    "            doc_mid2smask, \n",
    "            doc_tlinks, \n",
    "            task=task\n",
    "        )\n",
    "        deunk_toks += inst_deunk_toks\n",
    "        toks += inst_toks\n",
    "        sour_masks += inst_sour_masks\n",
    "        targ_masks += inst_targ_masks\n",
    "        sent_masks += inst_sent_masks\n",
    "        rels += inst_rels\n",
    "    return deunk_toks, toks, sour_masks, targ_masks, sent_masks, rels\n",
    "\n",
    "\n",
    "def convert_to_np_v2(deunk_toks, toks, sour_masks, targ_masks, sent_masks, labs, lab2ix):\n",
    "    max_len = max([len(t) for t in toks])\n",
    "    logger.info('max seq length %i' % (max_len))\n",
    "    pad_tok_ids, pad_masks, pad_sm, pad_tm, pad_sent_m = [], [], [], [], []\n",
    "    for inst_tok, inst_sm, inst_tm, inst_sent_m, inst_lab in zip(toks, sour_masks, targ_masks, sent_masks, labs):\n",
    "        pad_inst_tok = padding_1d(['[CLS]'] + inst_tok, max_len + 1, pad_tok='[PAD]')\n",
    "        pad_inst_tok_ids = tokenizer.convert_tokens_to_ids(pad_inst_tok)\n",
    "        pad_inst_masks = padding_1d([1] * (len(inst_tok) + 1), max_len + 1, pad_tok=0)\n",
    "        pad_inst_sm = padding_1d([0] + inst_sm, max_len + 1, pad_tok=0)\n",
    "        pad_inst_tm = padding_1d([0] + inst_tm, max_len + 1, pad_tok=0)\n",
    "        pad_inst_sent_m = padding_1d([0] + inst_sent_m, max_len + 1, pad_tok=0)\n",
    "        pad_tok_ids.append(pad_inst_tok_ids)\n",
    "        pad_masks.append(pad_inst_masks)\n",
    "        pad_sm.append(pad_inst_sm)\n",
    "        pad_tm.append(pad_inst_tm)\n",
    "        pad_sent_m.append(pad_inst_sent_m)\n",
    "    lab_ids = [lab2ix[l] for l in labs]\n",
    "    assert len(pad_tok_ids) == len(pad_masks) == len(pad_sm) == len(pad_tm) == len(pad_sent_m) == len(lab_ids)\n",
    "    return np.array(pad_tok_ids), np.array(pad_masks), np.array(pad_sm), np.array(pad_tm), np.array(pad_sent_m), np.array(lab_ids)\n",
    "\n",
    "\n",
    "def doc_kfold(data_dir):\n",
    "    file_list, data_splits = [], []\n",
    "    for file in sorted(os.listdir(data_dir)):\n",
    "        if file.endswith(\".xml\"):\n",
    "            dir_file = os.path.join(data_dir, file)\n",
    "            file_list.append(dir_file)\n",
    "    logger.info(\"[Number] %i files in '%s'\" % (len(file_list), data_dir))\n",
    "    gss = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_split, test_split in gss.split(file_list):\n",
    "        data_splits.append((train_split.tolist(), test_split.tolist()))\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-09 23:37:56,193 - Data_Process - INFO - [Number] 54 files in 'data/merge/BCCWJ-TIMEX'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(description='Bert-based Temporal Relation Classifier')\n",
    "\n",
    "parser.add_argument(\"-t\", \"--task\", dest=\"task\",\n",
    "                    help=\"classification task, i.g. DCT, T2E, E2E and MAT\")\n",
    "parser.add_argument(\"-l\", \"--lab\", dest=\"lab_type\",\n",
    "                    help=\"lab_type, i.g. 4c, 6c or None\")\n",
    "parser.add_argument(\"-c\", \"--comp\", dest=\"comp\",\n",
    "                    help=\"complete match, True or False\", type=bool)\n",
    "args = parser.parse_args()\n",
    "\n",
    "logger.info('[args] task: %s, label type: %s, complete agree: %s' % (\n",
    "    args.task, \n",
    "    args.lab_type,\n",
    "    str(args.comp)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/merge/BCCWJ-TIMEX'\n",
    "\n",
    "data_splits = doc_kfold(data_dir)\n",
    "\n",
    "deunk_toks, toks, sour_masks, targ_masks, sent_masks, labs = batch_make_tlink_instances_v2(\n",
    "    data_dir, \n",
    "    task=args.task,\n",
    "    lab_type=args.lab_type,\n",
    "    comp=args.comp\n",
    ")\n",
    "logger.info('Full data size %i ...' % len(labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_xml_to_brat('data/merge/BCCWJ-TIMEX/00001_A_PN1c_00001.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'data/merge/BCCWJ-TIMEX'\n",
    "# for file in os.listdir(data_dir):\n",
    "#     if file.endswith(\".xml\"):\n",
    "#         dir_file = os.path.join(data_dir, file)\n",
    "#         convert_xml_to_brat(dir_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_deunk_toks, doc_toks, doc_mid2smask, doc_tlinks = extract_sents_from_xml_v2(\n",
    "#     'data/merge/BCCWJ-TIMEX/00003_A_PN1e_00001.xml'\n",
    "# )\n",
    "# deunk_toks, toks, sour_masks, targ_masks, sent_masks, rels = make_tlink_instances_v2(doc_deunk_toks, doc_toks, doc_mid2smask, doc_tlinks, task='MAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deunk_toks, toks, sour_masks, targ_masks, labs = batch_make_tlink_instances(\n",
    "#     'data/merge/BCCWJ-TIMEX', \n",
    "#     task='DCT', \n",
    "#     verbose=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2ix = get_label2ix(labs)\n",
    "lab2count = {}\n",
    "for l in labs:\n",
    "    if l not in lab2count:\n",
    "        lab2count[l] = 1\n",
    "    else:\n",
    "        lab2count[l] += 1\n",
    "logger.info(str(lab2count))\n",
    "logger.info('major vote: %.2f%%' % (100 * max(lab2count.values()) / sum(lab2count.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_ids_np, tok_masks_np, sour_masks_np, targ_masks_np, sent_masks_np, lab_ids_np = convert_to_np_v2(\n",
    "    deunk_toks, \n",
    "    toks, \n",
    "    sour_masks, \n",
    "    targ_masks, \n",
    "    sent_masks, \n",
    "    labs,\n",
    "    lab2ix\n",
    ")\n",
    "logger.info(str(toks_ids_np.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toks_ids_np, tok_masks_np, sour_masks_np, targ_masks_np, lab_ids_np = convert_to_np(toks, \n",
    "#                                                                                     sour_masks, \n",
    "#                                                                                     targ_masks, \n",
    "#                                                                                     labs, \n",
    "#                                                                                     lab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_ids_np = np.array(generate_group_ids(toks_ids_np))  # those tlinks given the same sentences are grouped\n",
    "# print(group_ids_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "train_split, test_split = list(gss.split(toks_ids_np, lab_ids_np))[0]\n",
    "logger.info('train size: %i, test size: %i' % (len(train_split), len(test_split)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "train_tensors = TensorDataset(\n",
    "    torch.from_numpy(toks_ids_np[train_split]).to(device),\n",
    "    torch.from_numpy(tok_masks_np[train_split]).to(device),\n",
    "    torch.from_numpy(sour_masks_np[train_split]).to(device),\n",
    "    torch.from_numpy(targ_masks_np[train_split]).to(device),\n",
    "    torch.from_numpy(sent_masks_np[train_split]).to(device),\n",
    "    torch.from_numpy(lab_ids_np[train_split]).to(device)\n",
    ")\n",
    "test_tensors = TensorDataset(\n",
    "    torch.from_numpy(toks_ids_np[test_split]).to(device),\n",
    "    torch.from_numpy(tok_masks_np[test_split]).to(device),\n",
    "    torch.from_numpy(sour_masks_np[test_split]).to(device),\n",
    "    torch.from_numpy(targ_masks_np[test_split]).to(device),\n",
    "    torch.from_numpy(sent_masks_np[test_split]).to(device),\n",
    "    torch.from_numpy(lab_ids_np[test_split]).to(device)\n",
    ")\n",
    "train_dataloader = DataLoader(train_tensors, batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_dataloader = DataLoader(test_tensors, batch_size=BATCH_SIZE,shuffle=False)\n",
    "logger.info('Train batch num: %i, Test batch num: %i' % (len(train_dataloader), len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RelationClassifier.from_pretrained(BERT_URL, num_labels=len(lab2ix))\n",
    "model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer= BertAdam(optimizer_grouped_parameters,\n",
    "                    lr=5e-5,\n",
    "                    warmup=0.1,\n",
    "                    t_total=NUM_EPOCHS * len(train_dataloader))\n",
    "\n",
    "eval_lab(model, test_dataloader, 0)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    for (b_tok, b_mask, b_sour_mask, b_targ_mask, b_sent_mask, b_lab) in tqdm(train_dataloader):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        loss = model(b_tok, b_sour_mask, b_targ_mask, token_type_ids=b_sent_mask, attention_mask=b_mask, labels=b_lab)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    eval_lab(model, test_dataloader, epoch)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bert_env] *",
   "language": "python",
   "name": "conda-env-bert_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
